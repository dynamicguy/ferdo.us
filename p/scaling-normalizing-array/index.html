<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="In this post, we‚Äôll explore what scaling and normalization mean, the key techniques, when to use each, and how to implement them in Python using NumPy and scikit-learn."><title>Scaling and Normalizing Arrays - A Practical Guide for Data Preprocessing</title><link rel=canonical href=https://ferdo.us/p/scaling-normalizing-array/><link rel=stylesheet href=/scss/style.min.b0e76e98cdd4f7082d1fc36f7af7e0843a81201db2197c03ca426751dc923735.css><meta property='og:title' content="Scaling and Normalizing Arrays - A Practical Guide for Data Preprocessing"><meta property='og:description' content="In this post, we‚Äôll explore what scaling and normalization mean, the key techniques, when to use each, and how to implement them in Python using NumPy and scikit-learn."><meta property='og:url' content='https://ferdo.us/p/scaling-normalizing-array/'><meta property='og:site_name' content='Programming with Ferdous'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='scaling'><meta property='article:tag' content='normalizing'><meta property='article:tag' content='array'><meta property='article:tag' content='DataScience'><meta property='article:tag' content='MachineLearning'><meta property='article:tag' content='Python'><meta property='article:tag' content='NumPy'><meta property='article:tag' content='Preprocessing'><meta property='article:published_time' content='2025-11-16T00:00:00+00:00'><meta property='article:modified_time' content='2025-11-16T00:00:00+00:00'><meta property='og:image' content='https://ferdo.us/p/scaling-normalizing-array/cover.jpg'><meta name=twitter:title content="Scaling and Normalizing Arrays - A Practical Guide for Data Preprocessing"><meta name=twitter:description content="In this post, we‚Äôll explore what scaling and normalization mean, the key techniques, when to use each, and how to implement them in Python using NumPy and scikit-learn."><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://ferdo.us/p/scaling-normalizing-array/cover.jpg'><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu_435910bcc13e8c4.jpg width=300 height=375 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>üç•</span></figure><div class=site-meta><h1 class=site-name><a href=/>Programming with Ferdous</a></h1><h2 class=site-description>Let's build things together</h2></div></header><ol class=menu-social><li><a href=https://github.com/ferdo-us target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://twitter.com/ferdous target=_blank title=Twitter rel=me><svg class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li><li><a href=https://youtube.com/nurulferdous target=_blank title=Youtube rel=me><svg fill="#949494" height="24" width="24" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 512 512" stroke="currentColor"><g id="SVGRepo_bgCarrier" stroke-width="0"/><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round"/><g id="SVGRepo_iconCarrier"><g><g><path d="M435.574 59.858H76.426C34.285 59.858.0 94.143.0 136.284v171.023c0 4.427 3.589 8.017 8.017 8.017 4.427.0 8.017-3.589 8.017-8.017V136.284c0-33.3 27.092-60.393 60.393-60.393h359.148c33.3.0 60.393 27.092 60.393 60.393v239.432c0 33.3-27.092 60.393-60.393 60.393H76.426c-33.3.0-60.393-27.092-60.393-60.393v-34.205c0-4.427-3.589-8.017-8.017-8.017-4.427.0-8.017 3.589-8.017 8.017v34.205c0 42.141 34.285 76.426 76.426 76.426h359.148c42.141.0 76.426-34.285 76.426-76.426V136.284C512 94.143 477.715 59.858 435.574 59.858z"/></g></g><g><g><path d="M362.982 249.278l-34.205-22.233c-3.712-2.412-8.677-1.359-11.091 2.353-2.412 3.712-1.36 8.677 2.353 11.091L343.903 256l-148.296 96.394V159.607l98.779 64.206c3.711 2.411 8.678 1.359 11.09-2.353 2.414-3.712 1.36-8.677-2.353-11.091l-111.165-72.256c-5.24-3.407-12.384.491-12.384 6.721v222.33c0 6.226 7.142 10.131 12.385 6.721L362.982 262.72C367.761 259.615 367.76 252.385 362.982 249.278z"/></g></g></g></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#why-scale-or-normalize>Why Scale or Normalize?</a></li><li><a href=#key-techniques>Key Techniques</a><ol><li><a href=#1-min-max-scaling-normalization-to->1. <strong>Min-Max Scaling (Normalization to [0,1])</strong></a><ol><li><a href=#python-example-numpy>Python Example (NumPy)</a></li><li><a href=#with-scikit-learn>With scikit-learn</a></li></ol></li><li><a href=#2-standardization-z-score-normalization>2. <strong>Standardization (Z-score Normalization)</strong></a><ol><li><a href=#python-example>Python Example</a></li><li><a href=#with-scikit-learn-1>With scikit-learn</a></li></ol></li><li><a href=#3-robust-scaling>3. <strong>Robust Scaling</strong></a></li><li><a href=#4-max-absolute-scaling>4. <strong>Max Absolute Scaling</strong></a></li></ol></li><li><a href=#comparison-table>Comparison Table</a></li><li><a href=#best-practices>Best Practices</a></li><li><a href=#real-world-example-scaling-before-pca>Real-World Example: Scaling Before PCA</a></li><li><a href=#conclusion>Conclusion</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/scaling-normalizing-array/><img src=/p/scaling-normalizing-array/cover_hu_12d4bd432d7c07b0.jpg srcset="/p/scaling-normalizing-array/cover_hu_12d4bd432d7c07b0.jpg 800w, /p/scaling-normalizing-array/cover_hu_b74c278070503928.jpg 1600w" width=800 height=450 loading=lazy alt="Featured image of post Scaling and Normalizing Arrays - A Practical Guide for Data Preprocessing"></a></div><div class=article-details><header class=article-category><a href=/categories/machine-learning/ style=background-color:#fc036f;color:#fff>Machine Learning</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/scaling-normalizing-array/>Scaling and Normalizing Arrays - A Practical Guide for Data Preprocessing</a></h2><h3 class=article-subtitle>In this post, we‚Äôll explore what scaling and normalization mean, the key techniques, when to use each, and how to implement them in Python using NumPy and scikit-learn.</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published datetime=2025-11-16T00:00:00Z>Nov 16, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>4 minute read</time></div></footer></div></header><section class=article-content><h1 id=scaling-and-normalizing-arrays-a-practical-guide-for-data-preprocessing>Scaling and Normalizing Arrays: A Practical Guide for Data Preprocessing</h1><p>In machine learning, data analysis, and scientific computing, raw data often comes in different scales. One feature might range from 0 to 1, while another spans 1,000 to 100,000. Feeding such mismatched data into algorithms can lead to poor performance, slow convergence, or biased results. This is where <strong>scaling</strong> and <strong>normalizing</strong> arrays come in‚Äîessential preprocessing steps that bring features to a common scale.</p><p>In this post, we‚Äôll explore what scaling and normalization mean, the key techniques, when to use each, and how to implement them in Python using NumPy and scikit-learn.</p><hr><h2 id=why-scale-or-normalize>Why Scale or Normalize?</h2><p>Most distance-based algorithms (like K-Means, KNN, SVM) and gradient-based methods (like neural networks, logistic regression) are sensitive to the magnitude of features. Without scaling:</p><ul><li>Features with larger ranges dominate the model.</li><li>Training becomes unstable or slow.</li><li>Interpretability of coefficients (e.g., in linear models) is compromised.</li></ul><p><strong>Goal</strong>: Make features comparable without distorting differences in ranges of values.</p><hr><h2 id=key-techniques>Key Techniques</h2><h3 id=1-min-max-scaling-normalization-to->1. <strong>Min-Max Scaling (Normalization to [0,1])</strong></h3><p>Transforms data to a fixed range, usually [0, 1].</p>$$
X_{\text{scaled}} = \frac{X - X_{\min}}{X_{\max} - X_{\min}}
$$<ul><li><strong>When to use</strong>: When you need bounded values (e.g., neural networks with sigmoid/tanh activations, image pixel normalization).</li><li><strong>Pros</strong>: Preserves the original distribution shape; interpretable.</li><li><strong>Cons</strong>: Sensitive to outliers (one extreme value compresses all others).</li></ul><h4 id=python-example-numpy>Python Example (NumPy)</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>min_max_scale</span><span class=p>(</span><span class=n>arr</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>(</span><span class=n>arr</span> <span class=o>-</span> <span class=n>arr</span><span class=o>.</span><span class=n>min</span><span class=p>())</span> <span class=o>/</span> <span class=p>(</span><span class=n>arr</span><span class=o>.</span><span class=n>max</span><span class=p>()</span> <span class=o>-</span> <span class=n>arr</span><span class=o>.</span><span class=n>min</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>15</span><span class=p>,</span> <span class=mi>20</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>X_scaled</span> <span class=o>=</span> <span class=n>min_max_scale</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>X_scaled</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># Output: [0.   0.21 0.47 0.74 1.  ]</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=with-scikit-learn>With scikit-learn</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>MinMaxScaler</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>scaler</span> <span class=o>=</span> <span class=n>MinMaxScaler</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([[</span><span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>5</span><span class=p>],</span> <span class=p>[</span><span class=mi>10</span><span class=p>],</span> <span class=p>[</span><span class=mi>15</span><span class=p>],</span> <span class=p>[</span><span class=mi>20</span><span class=p>]])</span>
</span></span><span class=line><span class=cl><span class=n>X_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>X_scaled</span><span class=o>.</span><span class=n>flatten</span><span class=p>())</span>
</span></span></code></pre></td></tr></table></div></div><hr><h3 id=2-standardization-z-score-normalization>2. <strong>Standardization (Z-score Normalization)</strong></h3><p>Centers data around mean 0 with standard deviation 1.</p>$$
X_{\text{standardized}} = \frac{X - \mu}{\sigma}
$$<ul><li><strong>When to use</strong>: Most common choice. Ideal for algorithms assuming Gaussian distribution (e.g., linear regression, logistic regression, PCA).</li><li><strong>Pros</strong>: Robust to outliers; works well with gradient descent.</li><li><strong>Cons</strong>: Doesn‚Äôt bound values (can be negative or >1).</li></ul><h4 id=python-example>Python Example</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>standardize</span><span class=p>(</span><span class=n>arr</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>(</span><span class=n>arr</span> <span class=o>-</span> <span class=n>arr</span><span class=o>.</span><span class=n>mean</span><span class=p>())</span> <span class=o>/</span> <span class=n>arr</span><span class=o>.</span><span class=n>std</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>15</span><span class=p>,</span> <span class=mi>20</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>X_std</span> <span class=o>=</span> <span class=n>standardize</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>X_std</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># Output: [-1.42 -0.66 -0.08  0.66  1.42]</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=with-scikit-learn-1>With scikit-learn</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>StandardScaler</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>scaler</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>X_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>X_scaled</span><span class=o>.</span><span class=n>flatten</span><span class=p>())</span>
</span></span></code></pre></td></tr></table></div></div><hr><h3 id=3-robust-scaling>3. <strong>Robust Scaling</strong></h3><p>Uses median and interquartile range (IQR) instead of mean/std.</p>$$
X_{\text{robust}} = \frac{X - \text{median}}{IQR}
$$<ul><li><strong>When to use</strong>: Data with outliers.</li><li><strong>Pros</strong>: Outlier-resistant.</li><li><strong>Cons</strong>: Less intuitive scaling.</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>RobustScaler</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>scaler</span> <span class=o>=</span> <span class=n>RobustScaler</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>X_robust</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><hr><h3 id=4-max-absolute-scaling>4. <strong>Max Absolute Scaling</strong></h3><p>Scales by dividing by the maximum absolute value.</p>$$
X_{\text{scaled}} = \frac{X}{|X|_{\max}}
$$<ul><li>Range: [-1, 1]</li><li>Great for sparse data (e.g., text TF-IDF).</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>MaxAbsScaler</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>scaler</span> <span class=o>=</span> <span class=n>MaxAbsScaler</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>X_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><hr><h2 id=comparison-table>Comparison Table</h2><div class=table-wrapper><table><thead><tr><th>Method</th><th>Range</th><th>Outlier Robust?</th><th>Use Case</th></tr></thead><tbody><tr><td>Min-Max</td><td>[0,1] or custom</td><td>No</td><td>Neural nets, bounded inputs</td></tr><tr><td>Standardization</td><td>~[-3,3]</td><td>Moderate</td><td>Most ML algorithms</td></tr><tr><td>Robust Scaling</td><td>Varies</td><td>Yes</td><td>Outlier-heavy data</td></tr><tr><td>MaxAbs Scaling</td><td>[-1,1]</td><td>Yes</td><td>Sparse data</td></tr></tbody></table></div><hr><h2 id=best-practices>Best Practices</h2><ol><li><p><strong>Fit on Training Data Only</strong><br>Never use test data statistics to avoid data leakage.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>scaler</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>X_train_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>X_test_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>  <span class=c1># Only transform!</span>
</span></span></code></pre></td></tr></table></div></div></li><li><p><strong>Apply Same Transformation to All Splits</strong><br>Consistency is key.</p></li><li><p><strong>Inverse Transform When Needed</strong><br>Convert predictions back to original scale.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>X_original</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>inverse_transform</span><span class=p>(</span><span class=n>X_scaled</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div></li><li><p><strong>Handle 1D vs 2D Arrays</strong><br>scikit-learn expects 2D input (<code>n_samples √ó n_features</code>).</p></li></ol><hr><h2 id=real-world-example-scaling-before-pca>Real-World Example: Scaling Before PCA</h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.decomposition</span> <span class=kn>import</span> <span class=n>PCA</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>StandardScaler</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Generate sample data</span>
</span></span><span class=line><span class=cl><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=mi>5</span><span class=p>)</span> <span class=o>*</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>100</span><span class=p>,</span> <span class=mi>1000</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>]</span> <span class=o>+</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>50</span><span class=p>,</span> <span class=mi>500</span><span class=p>,</span> <span class=mi>5000</span><span class=p>,</span> <span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Without scaling: high-variance features dominate</span>
</span></span><span class=line><span class=cl><span class=n>pca_raw</span> <span class=o>=</span> <span class=n>PCA</span><span class=p>(</span><span class=n>n_components</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>X_pca_raw</span> <span class=o>=</span> <span class=n>pca_raw</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># With scaling</span>
</span></span><span class=line><span class=cl><span class=n>scaler</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>X_scaled</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>pca</span> <span class=o>=</span> <span class=n>PCA</span><span class=p>(</span><span class=n>n_components</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>X_pca</span> <span class=o>=</span> <span class=n>pca</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X_scaled</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Explained variance (scaled):&#34;</span><span class=p>,</span> <span class=n>pca</span><span class=o>.</span><span class=n>explained_variance_ratio_</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><blockquote><p><strong>Result</strong>: Scaling ensures all features contribute fairly to principal components.</p></blockquote><hr><h2 id=conclusion>Conclusion</h2><p><strong>Scaling ‚â† Normalization</strong> ‚Äî though often used interchangeably:</p><ul><li><strong>Normalization</strong> typically refers to rescaling to a norm (e.g., unit norm or [0,1]).</li><li><strong>Scaling</strong> is a broader term including standardization.</li></ul><p><strong>Rule of Thumb</strong>:</p><ul><li>Use <strong>StandardScaler</strong> by default.</li><li>Use <strong>MinMaxScaler</strong> when you need bounded values.</li><li>Use <strong>RobustScaler</strong> if outliers are a concern.</li></ul><p>Proper scaling is a small step that leads to giant leaps in model performance.</p><hr><p><strong>Your Turn</strong>: What‚Äôs your go-to scaler? Drop a comment or tweet <a class=link href=https://x.com/ferdous target=_blank rel=noopener>@ferdous</a>!</p><p><em>Happy preprocessing!</em></p><hr></section><footer class=article-footer><section class=article-tags><a href=/tags/scaling/>Scaling</a>
<a href=/tags/normalizing/>Normalizing</a>
<a href=/tags/array/>Array</a>
<a href=/tags/datascience/>DataScience</a>
<a href=/tags/machinelearning/>MachineLearning</a>
<a href=/tags/python/>Python</a>
<a href=/tags/numpy/>NumPy</a>
<a href=/tags/preprocessing/>Preprocessing</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>Last updated on Nov 16, 2025 00:00 UTC</span></section></footer></article><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//ferdo-us.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2025 Programming with Ferdous</section><section class=powerby>We use Google Analytics to collect website traffic data. You consent to use GA by using this site.</section><script async src="https://www.googletagmanager.com/gtag/js?id=G-XL72EN01QB"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-XL72EN01QB")</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.css integrity=sha384-WcoG4HRXMzYzfCgiyfrySxx90XSl2rxY5mnVY5TwtWE6KLrArNKn0T/mOgNL0Mmi crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.js integrity=sha384-J+9dG2KMoiR9hqcFao0IBLwxt6zpcyN68IgwzsCSkbreXUjmNVRhPFTssqdSGjwQ crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous onload=renderMathInElement(document.body)></script></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.c922af694cc257bf1ecc41c0dd7b0430f9114ec280ccf67cd2c6ad55f5316c4e.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>